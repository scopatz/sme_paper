\documentclass[preprint,12pt]{elsarticle}

%General Short-Cut Commands
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\subscript}[1]{\ensuremath{_{\textrm{#1}}}}
\newcommand{\nuc}[2]{\superscript{#2}{#1}}
\newcommand{\ith}[0]{$i$\superscript{th}}
\newcommand{\jth}[0]{$j$\superscript{th}}
\newcommand{\kth}[0]{$k$\superscript{th}}

\usepackage{natbib}
%\bibpunct[,]{[}{]}{;}{a}{,}{,}
\bibliographystyle{model2-names}

% Needed for making certian figures
\usepackage{color}
\usepackage{tikz}
\usepackage{verbatim}
\usetikzlibrary{shapes,arrows}

\journal{Some Separations Journal}

% Start this document...
\begin{document}

% Article commands
\begin{frontmatter}
\title{Symbolic Multicomponent Enrichment for Matched Abundance Ratio Cascades}

\author[chi]{Anthony M. Scopatz\corref{cor1}}
\ead{scopatz@flash.uchicago.edu}
\cortext[cor1]{Corresponding author}

\address[chi]{The University of Chicago, The FLASH Center, 
              5754 S. Ellis Ave, Chicago, IL, 60637}


% Text of abstract
\begin{abstract}
Some abstract text
\end{abstract}

%% keywords here, in the form: keyword \sep keyword
\begin{keyword}
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}


%
% Begin real text
%

\section{Introduction}
\label{sec:intro}
Symbolic evaluation is slow, except...


\section{Symbolic Methodology}
\label{sec:meth}
At its core, the solution to coupled multicomponent enrichment cascade equations is
a minimization problem \cite{Wood1999}.  In a matched abundance ratio 
\cite{DelaGarza1969} cascade this is often cast as the minimization of 
the total flow rate $L$ through the system normalized by the feed flow rate $F$ to 
form the positive and unitless optimization variable $L/F$.  Traditionally, 
minimizing this parameter as a function of other cascade state variables has been 
performed eiether via off-the-shelf optimization libraries 
\cite{doi:10.1080/01496391003793884} or custom built solvers CITEpyne.

However, both of these approaches rely on writing code idiomatic to underlying 
programming language.  For example, many numerical constraint optimizers, in order 
to handle the case of arbiratily complex expressions, accept as their primary 
argument a function object or pointer.  Then for every iteration the optimizer must 
call the function to be optimized.  This involves (in compiled languages) setting up 
a new stack and allocating new memory for every solver iteration.  While good 
optimizers will be effieicnt in terms of the number of function calls they make, 
there is no way for them to avoid multiple function calls and remain sufficiently 
general. The number of function calls that are needed scales \emph{at least} linearly 
with the number of equations in the system.

On the other hand, if it is possible to convert the system of equations 
into a single expression -- no matter how complicated -- then the problem has been 
recast into the optimization of a single fucntion over a single variable.   This 
may be easily handed off to any number of well known algorithims, such as Newton's
method, the secant method, or the bisection method.  In such a form, the solution 
will have a vastly reduced computational overhead as compared to standard 
numerical solvers. 

Naturally, performing the above variable substitution and elimination to obtain a 
single expression is difficult in many cases and mathematically impossible in most 
others.  For MARC systems, as is shown in \S \ref{sec:symes}, reduction of $L/F$
to a single expression is possible.

Rather than phrasing this as a numerical optimization problem from the start, the 
opposite conceptual tactic is taken.  The MARC system is expandend, manipulated, 
and subsititued symbolically the the majority of the calculation.  The actual 
numerical results of the calculation are only computed as needed as the final step.
The symbolic manipulation is generated prior to compilation, resulting in 
very fast run time solutions.

In \S \ref{sec:marceq} the standard MARC system of equations is detailed. 
Then the novel reinterpretation of the MARC equations is related in 
\S \ref{sec:symes}.  
Lastly, \S \ref{sec:codegen} characterizes the pipeline that takes the symbolic MARC 
eqautions and converts them into fast machine code.  
For the remainder of \S \ref{sec:meth}, a three-component natural uranium fed 
cascade will be used when example data is required.
Higher order cascades will be discussed in \S \ref{sec:res}.

\subsection{MARC Equations}
\label{sec:marceq}
The algorithm with which to compute matched abunance ratio cascades has been 
detailed previously in a vareity sources, dating back to de la Garza in the
1960s. However, the fundementals of this method are reviewed here both as a formal
statement of the problem and as a way to introduce the terminology used throughout 
the rest of this paper.  For more information please refer to de la Garza
\cite{DelaGarza1969}, von Halle \cite{VonHalle1987}, Wood \cite{Wood1999}, or 
Song \cite{doi:10.1080/01496391003793884}.

Call $N$ the total number of stages in an enrichment cascade, $N_P$ the number of 
product (enriching) stages above the feed entry, and $N_T$ the number of tails
(stripping) stages below the feed point such that:
\begin{equation}
N = N_P + N_T
\end{equation}
Furthermore, MARC requires that we declare a species to enrich in the 
product (subscript P).  This is known as the first key component an is denoted 
here as $j$.  There must also be a second key component $k$ from which $j$ is 
separated. Thus $k$ is more abundant in the tails stream (subscript T).

The ovearll stage separation factor $\alpha$ [unitless] represents the per mass 
enrichment ratio from one stage to the next.  This yeilds component-specific
stage separation factors $\beta_i$ for the \ith component of a mixture $I$:
\begin{equation}
\beta_i = \alpha^{M^* - M_i}
\label{beta_i}
\end{equation}
Here, $M_i$ represents the molecular weight [amu] of the \ith
species while $M^*$ [amu] is an optimization parameter.  An initial first guess for 
$M^*$ is given by the average of the molecular weights of the key components:
\begin{equation}
M^* = \frac{1}{2}\left(M_j + M_k\right)
\label{mstar-guess}
\end{equation}
By convention, $j$ \& $k$ are chosen such that $M_j < M_k$ and 
therefore $M^*$ is bounded on the range $M^*\in[M_j,M_k]$ when minimizing $L/F$.

Let $F$, $P$, and $T$ be the feed, product, and tails mass flow rates subject to 
the conservation equation:
\begin{equation}
F = P + T
\label{total-flow-constraint}
\end{equation}
Also let $x$ denote a normalized mass concentration vector such that $x_i^F$, 
$x_i^P$, $x_i^T$ are the concetrations of the \ith component of the feed, product,
and tails respectively.  Thus:
\begin{equation}
1 = \sum_i^I x_i^F = \sum_i^I x_i^P = \sum_i^I x_i^T 
\end{equation}
Therefore the flow rate ratios are:
\begin{equation}
\frac{P}{F} = \sum_i^I x_i^F\frac{\beta_i^{N_T+1} - 1}
                                 {\beta_i^{N_T+1} - \beta_i^{-N_P}}
\label{ppf-full}
\end{equation}
\begin{equation}
\frac{T}{F} = \sum_i^I x_i^F\frac{1 - \beta_i^{-N_P}}
                                 {\beta_i^{N_T+1} - \beta_i^{-N_P}}
\label{tpf-full}
\end{equation}
Equations \ref{ppf-full} \&  \ref{tpf-full} can be further reduced to functions
of just the first key component:
\begin{equation}
\frac{P}{F} = \frac{x_j^F - x_j^T}{x_j^P - x_j^T}
\label{ppf-key}
\end{equation}
\begin{equation}
\frac{T}{F} = \frac{x_j^F - x_j^P}{x_j^T - x_j^P}
\label{tpf-key}
\end{equation}

Either equations \ref{ppf-full} \& \ref{tpf-full} or equations \ref{ppf-key} \& 
\ref{tpf-key} allow the calculation of the \ith component concentrations of in the 
product and tails streams as a function of the feed concentrations.
\begin{equation}
x_i^P = \frac{x_i^F}{\frac{P}{F}}\cdot\frac{\beta_i^{N_T+1} - 1}
                                           {\beta_i^{N_T+1} - \beta_i^{-N_P}}
\label{prod-concentration}
\end{equation}
\begin{equation}
x_i^T = \frac{x_i^F}{\frac{T}{F}}\cdot\frac{1 - \beta_i^{-N_P}}
                                           {\beta_i^{N_T+1} - \beta_i^{-N_P}}
\label{tail-concentration}
\end{equation}
By analogy to equation \ref{total-flow-constraint}, the components themselves are
also subject to mass conservation:
\begin{equation}
x_i^FF = x_i^PP + x_i^TT
\label{comp-flow-constraint}
\end{equation}

What gives MARCs their moniker, however, is that at every stage any two flows of 
the same type ($F$, $P$, \& $T$) must produce the same relative concentrations of 
the two key components.  Call $R$ an abundance ratio.  Then for matched abundance 
ratio cascades, 
\begin{equation}
R^F = \frac{x_j^F}{x_k^F}; \;\;\; R^P = \frac{x_j^P}{x_k^P}; \;\;\; 
R^T = \frac{x_j^T}{x_k^T}
\label{abund_ratios}
\end{equation}
The ratios in equation \ref{abund_ratios} are valid for the three stages in the 
cascade that are most important: the feed entry point, the final product, and the 
last tails stage.

Finally, the total flow rate per unit feed for a given cascade is computed by 
the following expression:
\begin{equation}
\frac{L}{F} = \sum_i^I \frac{\frac{P}{F}x_i^P\ln(R^P) + \frac{T}{F}x_i^T\ln(R^T) 
                                                      - x_i^F\ln(R^F)}
                            {\ln(\beta_j)\frac{\beta_i - 1.0}{\beta_i + 1.0}}
\label{ltot-over-feed}
\end{equation}
To solve for the optimal cascade, equation \ref{ltot-over-feed} must be minimized
with respect to its constituent independent variables.

Suppose that the cascade parameters 
$\alpha$, 
$j$, $k$, 
$M_i$ for all $i$, 
$x_i^F$ for all $i$, 
the key product enrichment $x_j^P$, and the key 
tails enrichment $x_j^T$ are all given.  The optimal cascade is thus characterized by 
the system of equations below. This system has three equations and three unknowns --
$N_P$, $N_T$, and $M^*$ -- and is therefore well determined.
\begin{equation}
\frac{x_j^P}{x_j^F}\cdot\frac{P}{F} - \frac{\beta_j^{N_T+1} - 1}
                                           {\beta_j^{N_T+1} - \beta_j^{-N_P}} = 0
\label{prod-constraint}
\end{equation}
\begin{equation}
\left(\frac{x_j^F}{\frac{T}{F}} \cdot \frac{1 - \beta_i^{-N_P}}
                                           {\beta_j^{N_T+1} - \beta_j^{-N_P}} \right)
- \left(x_j^T\cdot\sum_i^{I} x_i^T\right) = 0
\label{tail-constraint}
\end{equation}
\begin{equation}
\min\left[\frac{L}{F}\right]\to \frac{d}{dM^*} \frac{L}{F} = 0
\label{minlt-constraint}
\end{equation}
Equation \ref{prod-constraint} is a rearrangement of equation \ref{prod-concentration}
when evaluated for the \jth key component, whose product erichment is assumed
known.  Equation \ref{tail-constraint} is garnered via equations \ref{tpf-full} \&
\ref{tail-concentration} and once again evaluated for the \jth key component.
Equation \ref{minlt-constraint} falls out of the extreme value theorem of calculus
applied to equation \ref{ltot-over-feed}.

To solve equations \ref{prod-constraint}-\ref{minlt-constraint}, initial guesses
for the number of product and tails stages, $N_P^0$ and $N_T^0$, must be given.
An initial guess for $M*$ is provided in equation \ref{mstar-guess}.  Once this system
of equations is known, the product and tails concentrations for all components may be 
solved with equations \ref{prod-concentration} \& \ref{tail-concentration}.

\subsection{Symbolic Enrichment Solver}
\label{sec:symes}

When solving a system of equations, it is advantageous if any of the independent 
variables has a closed form representation in terms of the other independent 
parameters. This is because the closed form reprsentation may be subtituted into 
the remaining equations and this variable eliminated from the computation.  This
transorms the system from $X$ equations with $X$ unknowns into a system with 
$(X-1)$ equations and $(X-1)$  unknowns.  Ideally, the system would then be reducible 
to one equation (with potentiall many substitutions) as a function of one unknown.

The symbolic MARC algorithm here relies on eliminating the $N_P$ and $N_T$ variables
from equations \ref{prod-constraint}-\ref{minlt-constraint}. This produces a single
expression for $L/F$, which may be handed to a traditional minimization algorithm.
Still, the strategy for devising a successful symbolic solver for a given system
of equations is distinct from the normal mechnisms used when engineering numeric 
solutions.

Unlike numeric solvers where every expression is evaluated with an integer, real, or
complex number as it is encountered, symbolic computation represents and stores all 
opperations performed on an expression.  Only after the accumulation of all 
opperations, and only if desired by the user, is a symbolic expression ever numerically
evaluted.  This is true even for compound operators such as triginometric functions, 
differentiaition, and integration.

Therefore certain operators (e.g. differentiation) may explode the number of binary
opperations in the resultant expression after their application (e.g. the chain rule).
So while the quintessential strategy in numerical evaluators is to mitigate error
steming from floating point arithmetic, a symbolic solver seeks to minimize the total
number of irreducible operation ($+, -, \times, /, \%, \log, \sin, \mbox{abs},$ etc.) 
which must be perfomed to compute a complex expression.  Hence much of the variable
elimination below also attempts to simeltaneously curtail the overall operation count. 

To start, consider the constraint in equation \ref{prod-constraint} and the number
of tails stages $N_T$.  As simple call to SymPy's \texttt{solve()} function reveals
the closed form solution $N_T(N_P, M^*)$:
\begin{equation}
\begin{array}{lr}
\tiny
N_T = &
\frac{1}{\left(M^{*} - M_{j}\right) \log{\left (\alpha \right )}} \left[
- M^{*} N_{P} \log{\left (\alpha \right )} - M^{*} \log{\left (\alpha \right )} + M_{j} \log{\left (\alpha \right )} \right. \\ 
& \left. - \log{\left (x^{T}_{j} \right )}  - \log{\left (\frac{- x^{F}_{j} + x^{P}_{j}}{\alpha^{M^{*} N_{P}} x^{F}_{j} x^{P}_{j} - \alpha^{M^{*} N_{P}}x^{F}_{j} x^{T}_{j} - \alpha^{M_{j} N_{P}} x^{F}_{j} x^{P}_{j} + \alpha^{M_{j} N_{P}} x^{P}_{j} x^{T}_{j}} \right )} \right] \\
\end{array}
\end{equation}

\subsection{Code Generation Pipeline}
\label{sec:codegen}

\section{Results \& Profiling}
\label{sec:res}

\section{Conclusions \& Future Work}
\label{sec:conc}

\bibliography{refs}

\end{document}
